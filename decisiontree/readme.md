# 决策树算法
csdn博客总结 https://blog.csdn.net/qq_39751437/article/details/86550287

决策树是一种基本的分类回归方法，由结点和有向边组成，结点有两种类型，内部结点和叶节点，内部结点表示属性或特征，叶节点表示类别，呈树形结构决策树可以看成是定义在特征空间和类空间上的条件概率分布，也可以看成if-then规则集合，每一个实例都被一条路和一个规则所覆盖，而且只被一条覆盖。

## (1)算法思路：

三个步骤：特征选择–决策树的生成–决策树的剪枝


1、导入数据集，包括数据特征和标签

2、数据预处理（决策树基本不需要预处理，不需要提前归一化，处理缺失值。）

3、计算给定数据集香农熵

4、根据不同特征选择方式，选择特征（ID3决策树通常选择信息增益最大的特征来生成决策树。C4.5决策树通常选择信息增益比最大的特征来生成决策树。CART决策树通常选择基尼指数最小的特征来生成决策树。）

5、创建决策树

6、调整决策树参数，包括max_features，max_depth，min_samples_split。可用网格搜索进行参数优化

7、决策树可视化与存储

8、决策树预测

## (2)决策树算法本质

从if-then规则集合角度来看，决策树本质是从训练数据归纳出一组分类规则，可能有多种分类规则，选择与训练数据矛盾最小的规则，同时具有较好的泛化能力。
从条件概率角度来看，决策数本质是学习由训练数据集估计的条件概率模型，选择条件概率模型的依据是不仅能对训练数据有较好的拟合，而且对位置数据也有很好的预测。
从if-then规则集合角度来看，决策树本质是从训练数据归纳出一组分类规则，可能有多种分类规则，选择与训练数据矛盾最小的规则，同时具有较好的泛化能力。
从条件概率角度来看，决策数本质是学习由训练数据集估计的条件概率模型，选择条件概率模型的依据是不仅能对训练数据有较好的拟合，而且对位置数据也有很好的预测

## (3)常用算法

ID3

C4.5

CART

ID3决策树通常选择信息增益最大的特征来生成决策树。

C4.5决策树通常选择信息增益比最大的特征来生成决策树。

CART决策树通常选择基尼指数最小的特征来生成决策树。

## 代码简介
### 上面代码全在python3下运行

decision_lihang book.py 是李航统计学习方法中代码

sklearn_decisiontree.py 是使用sklearn调用sklearn.tree.DecisionTreeClassifier模型在iris 数据集上，包括决策树可视化,以及如何画出决策树随深度变化的training_score and test_score

trees_python.py 是机器学习实战源代码

