from sklearn.linear_model import LinearRegression
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
import numpy as np
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
from sklearn import tree
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression


iris=datasets.load_iris()
x=iris.data
y=iris.target
x_train,x_test,y_train,y_test =train_test_split(x,y,test_size=0.3,random_state=0)

model = LogisticRegression(penalty='l2',solver='liblinear').fit(x_train,y_train)
print ('train_score is',model.score(x_train,y_train))
print ('test_score is',model.score(x_test,y_test))
y_pred=model.predict(x_test)
print (y_pred)
print (accuracy_score(y_test,y_pred))
print (confusion_matrix(y_test, y_pred))
print (classification_report(y_test, y_pred))

print (model.decision_function(x_test))##预测样本的置信度分数。
print (model.predict_proba(x_test))##输出分类概率,返回每种类别的概率

##模型效果获取
print (model.coef_)
print (model.intercept_)
print (model.classes_)
print (model.n_iter_)
print (model.feature_importances_)

##结果
# train_score is 0.9428571428571428
# test_score is 0.8888888888888888
# [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 2 1 0 0 2 2 0 0 2 0 0 1 1 0 2 2 0 2 2 2 0
#  2 1 1 2 0 2 0 0]
# 0.8888888888888888
# [[16  0  0]
#  [ 0 13  5]
#  [ 0  0 11]]
#               precision    recall  f1-score   support

#            0       1.00      1.00      1.00        16
#            1       1.00      0.72      0.84        18
#            2       0.69      1.00      0.81        11

#    micro avg       0.89      0.89      0.89        45
#    macro avg       0.90      0.91      0.88        45
# weighted avg       0.92      0.89      0.89        45

# [[ -6.75038448  -1.42038975   3.07511172]
#  [ -3.80936657   0.57832138  -1.99764398]
#  [  4.82973733  -2.60335054 -11.36189873]
#  [ -7.96080114   0.17202446   2.12145016]
#  [  3.37132035  -1.63481849  -9.34222924]
#  [ -7.87824534  -1.66236478   4.00608073]
#  [  3.82615281  -1.96853808  -9.73847854]
#  [ -4.296216    -0.67175573  -1.54845788]
#  [ -4.76325324  -0.06336027  -1.32224545]
#  [ -3.27020715  -0.55203601  -2.27084507]
#  [ -6.983966     0.28641736   1.99865061]
#  [ -3.8659704   -1.00813873  -1.64491336]
#  [ -4.64276967  -0.14830511  -0.79524738]
#  [ -4.55972651  -0.37086479  -1.09954151]
#  [ -4.70313329  -0.50879008  -0.51151524]
#  [  3.89872121  -1.88272898  -9.8521738 ]
#  [ -4.41941922  -0.74082866  -0.62264974]
#  [ -4.51612826  -0.22153013  -0.27586726]
#  [  2.88109097  -1.30504907  -8.57415344]
#  [  4.41493216  -2.48579762 -10.67797387]
#  [ -6.02829326  -1.13748381   2.12607717]
#  [ -4.44876921  -1.03117451  -0.07316841]
#  [  2.45212919  -1.5427442   -8.05035561]
#  [  2.68538573  -1.20603146  -7.98924482]
#  [ -5.38659194  -0.73190007   0.48312532]
#  [  4.52174985  -2.27400361 -10.10848744]
#  [  2.90512071  -2.2082722   -8.62196787]
#  [ -3.72834163  -0.52806185  -1.83743349]
#  [ -2.61102192  -0.23161767  -2.12320884]
#  [  2.97018107  -1.81400443  -8.70079273]
#  [ -6.38158715  -0.78148067   1.4712617 ]
#  [ -4.52895656  -1.10664045   0.26067004]
#  [  3.58253241  -1.69876653  -9.79426326]
#  [ -5.37438715  -1.00657594   0.65316219]
#  [ -7.36718888  -0.76349566   2.86921263]
#  [ -3.64801813  -0.9046393   -0.68892141]
#  [  3.66087882  -1.95510901 -10.30346466]
#  [ -6.03566853  -0.32122586   1.25175801]
#  [ -3.69923532  -1.04363078  -1.29245247]
#  [ -3.55684342  -0.27587146  -1.7346601 ]
#  [ -6.63931966  -0.82274131   1.52467193]
#  [  3.40849183  -1.55295955  -9.084098  ]
#  [ -5.82058578  -1.31303354   0.68344967]
#  [  2.90981745  -2.1744894   -8.4170606 ]
#  [  3.92476917  -1.90212856 -10.36449285]]
# [[1.01514149e-03 1.68979201e-01 8.30005658e-01]
#  [2.77325788e-02 8.19480925e-01 1.52786497e-01]
#  [9.35029110e-01 6.49599289e-02 1.09613940e-05]
#  [2.42826242e-04 3.78006486e-01 6.21750688e-01]
#  [8.55530068e-01 1.44392382e-01 7.75503424e-05]
#  [3.31674458e-04 1.39625714e-01 8.60042611e-01]
#  [8.88670133e-01 1.11276323e-01 5.35439656e-05]
#  [2.55043944e-02 6.41745983e-01 3.32749623e-01]
#  [1.20406836e-02 6.88638840e-01 2.99320477e-01]
#  [7.38700934e-02 7.37322610e-01 1.88807297e-01]
#  [6.37270703e-04 3.93143013e-01 6.06219717e-01]
#  [4.56194208e-02 5.94554364e-01 3.59826216e-01]
#  [1.21738578e-02 5.90871697e-01 3.96954445e-01]
#  [1.54918724e-02 6.10805180e-01 3.73702948e-01]
#  [1.18337203e-02 4.94503800e-01 4.93662480e-01]
#  [8.81207645e-01 1.18745037e-01 4.73178353e-05]
#  [1.73972190e-02 4.72032429e-01 5.10570352e-01]
#  [1.21889097e-02 5.01444291e-01 4.86366800e-01]
#  [8.16008527e-01 1.83828694e-01 1.62779346e-04]
#  [9.27804894e-01 7.21734650e-02 2.16412807e-05]
#  [2.11120150e-03 2.13229371e-01 7.84659428e-01]
#  [1.52854667e-02 3.47633809e-01 6.37080724e-01]
#  [8.39172443e-01 1.60536913e-01 2.90644100e-04]
#  [8.02260156e-01 1.97449353e-01 2.90491573e-04]
#  [4.80754568e-03 3.42657721e-01 6.52534733e-01]
#  [9.13780839e-01 8.61815373e-02 3.76235130e-05]
#  [9.05288649e-01 9.45394087e-02 1.71942054e-04]
#  [4.41311801e-02 6.97581923e-01 2.58286897e-01]
#  [1.10795385e-01 7.16190840e-01 1.73013775e-01]
#  [8.71445327e-01 1.28402202e-01 1.52470486e-04]
#  [1.49660381e-03 2.78137892e-01 7.20365505e-01]
#  [1.29575568e-02 3.01583918e-01 6.85458526e-01]
#  [8.62825301e-01 1.37125244e-01 4.94555427e-05]
#  [4.95963072e-03 2.87800682e-01 7.07239687e-01]
#  [4.99076065e-04 2.51329991e-01 7.48170933e-01]
#  [3.91842132e-02 4.44765438e-01 5.16050349e-01]
#  [8.87138523e-01 1.12830980e-01 3.04974541e-05]
#  [1.98788282e-03 3.50207011e-01 6.47805107e-01]
#  [4.82868711e-02 5.20865634e-01 4.30847495e-01]
#  [4.55311864e-02 7.08255049e-01 2.46213764e-01]
#  [1.15828013e-03 2.70620166e-01 7.28221554e-01]
#  [8.47058443e-01 1.52842284e-01 9.92725272e-05]
#  [3.36245822e-03 2.41038014e-01 7.55599527e-01]
#  [9.02641955e-01 9.71476777e-02 2.10366816e-04]
#  [8.83029992e-01 1.16941615e-01 2.83930478e-05]]
# [[ 0.40093674  1.31024707 -2.09750955 -0.95694164]
#  [ 0.37732968 -1.39413979  0.41885057 -1.10535497]
#  [-1.66919225 -1.18195572  2.39508793  2.00963854]]
# [ 0.24944929  0.81140352 -0.97217812]
# [0 1 2]
# [7]